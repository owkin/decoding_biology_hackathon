{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Direct Inference Demo\n",
    "\n",
    "This notebook demonstrates how to use vLLM for direct offline inference without Docker.\n",
    "\n",
    "## ⚠️ Important Limitations\n",
    "\n",
    "**This notebook is for individual experimentation only:**\n",
    "- **Single User**: The model is tied to your Python kernel\n",
    "- **Not Shared**: Other team members cannot access it\n",
    "- **Resource Intensive**: Requires dedicated GPU memory per user\n",
    "- **Not Recommended for Teams**: Use `vllm_demo.ipynb` with Docker server for team collaboration\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Install vLLM and dependencies:\n",
    "   ```bash\n",
    "   ./install.sh\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from vllm import LLM, SamplingParams\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize vLLM for Direct Inference\n",
    "\n",
    "We'll use vLLM's direct inference capabilities for offline batch processing, which is more efficient than running a separate server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vLLM for direct inference\n",
    "print(\"Initializing vLLM model...\")\n",
    "llm = LLM(\n",
    "    model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    trust_remote_code=True,\n",
    "    max_model_len=4096,\n",
    "    gpu_memory_utilization=0.8\n",
    ")\n",
    "\n",
    "# Set up sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    max_tokens=1024\n",
    ")\n",
    "\n",
    "print(\"vLLM model loaded successfully!\")\n",
    "print(f\"Model: Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "print(f\"Max model length: 4096 tokens\")\n",
    "print(f\"GPU memory utilization: 80%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Completion Example\n",
    "\n",
    "Generate a single completion using vLLM's direct inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single completion using vLLM\n",
    "prompt = \"What is the role of proteins in biological systems?\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nGenerating completion...\")\n",
    "\n",
    "# Generate completion using vLLM\n",
    "outputs = llm.generate([prompt], sampling_params)\n",
    "output = outputs[0]\n",
    "\n",
    "# Extract the text from the response\n",
    "completion_text = output.outputs[0].text\n",
    "tokens_used = len(output.outputs[0].token_ids)\n",
    "\n",
    "print(f\"\\nCompletion: {completion_text}\")\n",
    "print(f\"\\nTokens used: {tokens_used}\")\n",
    "print(f\"Finish reason: {output.outputs[0].finish_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Example\n",
    "\n",
    "vLLM excels at batch processing - running multiple prompts at once is much more efficient and faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions for multiple prompts\n",
    "prompts = [\n",
    "    \"Explain the process of DNA replication.\",\n",
    "    \"What are the main functions of mitochondria?\",\n",
    "    \"How do enzymes work in biological reactions?\"\n",
    "]\n",
    "\n",
    "print(\"Generating batch completions...\")\n",
    "print(f\"Processing {len(prompts)} prompts\\n\")\n",
    "\n",
    "# Generate batch completions using vLLM\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# Display results\n",
    "total_tokens = 0\n",
    "for i, (prompt, output) in enumerate(zip(prompts, outputs)):\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Answer: {output.outputs[0].text}\")\n",
    "    tokens_used = len(output.outputs[0].token_ids)\n",
    "    total_tokens += tokens_used\n",
    "    print(f\"Tokens used: {tokens_used}\")\n",
    "\n",
    "print(f\"\\nTotal tokens used: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Template Example\n",
    "\n",
    "vLLM supports chat templates for conversational interactions. We can use the model's built-in chat template.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat template example using vLLM's chat interface\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load the tokenizer to apply chat template\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "# Prepare messages for chat\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a poetic biology tutor. Use analogies and paint a pretty picture.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Explain photosynthesis in simple terms.\"},\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "\n",
    "print(\"Chat Prompt:\")\n",
    "print(chat_prompt)\n",
    "print(\"\\nGenerating response...\")\n",
    "\n",
    "# Generate using vLLM\n",
    "outputs = llm.generate([chat_prompt], sampling_params)\n",
    "response = outputs[0].outputs[0].text\n",
    "\n",
    "print(\"\\nChat Response:\")\n",
    "print(f\"Assistant: {response}\")\n",
    "print(f\"Tokens used: {len(outputs[0].outputs[0].token_ids)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
