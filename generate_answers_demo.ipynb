{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample Questions Demo - Decoding Biology Hackathon\n",
        "\n",
        "This notebook demonstrates how to process the sample questions from the hackathon and generate answers in the required format.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "1. Start the vLLM Docker container:\n",
        "   ```bash\n",
        "   ./start_vllm_docker.sh\n",
        "   ```\n",
        "\n",
        "2. Make sure you have the sample questions file (`hackathon-train.json`) in the current directory\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from openai import OpenAI\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set up logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#7Configuration for batch processing\n",
        "BATCH_SIZE = 256  # Number of questions per batch. Increasing this will speed up the processing at the cost of using more memory\n",
        "MAX_TOKENS_PER_QUESTION = 2_000  # Max tokens per individual question\n",
        "TEMPERATURE = 0.7  # Model temp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize the OpenAI Client\n",
        "\n",
        "Connect to the vLLM server running in Docker.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the OpenAI client to connect to vLLM server\n",
        "client = OpenAI(\n",
        "    api_key=\"EMPTY\",  # vLLM doesn't require authentication\n",
        "    base_url=\"http://localhost:8000/v1\", #vLLM server URL, make sure you have the correct port\n",
        ")\n",
        "\n",
        "# Test the connection\n",
        "try:\n",
        "    models = list(client.models.list())\n",
        "    if models:\n",
        "        print(\"vLLM server is up and running!\")\n",
        "        print(f\"Available models: {[model.id for model in models]}\")\n",
        "        model_name = models[0].id\n",
        "    else:\n",
        "        raise Exception(\"No models available\")\n",
        "except Exception as e:\n",
        "    print(f\"Error connecting to vLLM server: {e}\")\n",
        "    print(\"Make sure the server is running by executing `./start_vllm_docker.sh`\")\n",
        "    raise e \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions\n",
        "\n",
        "Define utility functions for loading and processing questions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache for tokenizers to avoid reloading them\n",
        "_tokenizer_cache = {}\n",
        "\n",
        "def get_tokenizer(model: str) -> AutoTokenizer:\n",
        "    if model not in _tokenizer_cache:\n",
        "        _tokenizer_cache[model] = AutoTokenizer.from_pretrained(model)\n",
        "    return _tokenizer_cache[model]\n",
        "\n",
        "def load_questions(filename) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Load questions from a JSON file.\"\"\"\n",
        "    with open(filename, 'r') as f:\n",
        "        if filename.endswith('.jsonl'):\n",
        "            questions = [json.loads(line.strip()) for line in f if line.strip()]\n",
        "        else:\n",
        "            questions = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded {len(questions)} questions from {filename}\")\n",
        "    return questions\n",
        "\n",
        "def create_prompts(questions: List[Dict[str, Any]], model_name:str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Create prompts for a batch of questions using AutoTokenizer.\n",
        "    \n",
        "    Args:\n",
        "        questions: List of question data\n",
        "    \n",
        "    Returns:\n",
        "        List of formatted prompts\n",
        "    \"\"\"\n",
        "    tokenizer = get_tokenizer(model_name)\n",
        "    prompts = []\n",
        "    \n",
        "    for question_data in questions:\n",
        "        question = question_data['question']\n",
        "        options = question_data.get('options', {})\n",
        "        \n",
        "        # Parse options if they're a string\n",
        "        if isinstance(options, str):\n",
        "            try:\n",
        "                options = json.loads(options)\n",
        "            except:\n",
        "                options = {}\n",
        "        \n",
        "        # Create the prompt with dynamic options\n",
        "        options_text = \"\"\n",
        "        for key, value in options.items():\n",
        "            options_text += f\"{key}: {value}\\n\"\n",
        "        \n",
        "        # Determine the valid options for the system message\n",
        "        valid_options = list(options.keys())\n",
        "        options_list = \", \".join(valid_options)\n",
        "        \n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": f\"You are a biology expert. Answer the following multiple choice questions by selecting the correct option ({options_list}) and providing a brief explanation. Always format your answer as <answer>[letter]</answer>.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"\"\"Question: {question}\n",
        "\n",
        "Options:\n",
        "{options_text}\n",
        "Please provide your answer as a single letter ({options_list}) followed by a brief explanation.\n",
        "Format your answer as: <answer>[letter]</answer>\n",
        "\n",
        "Answer:\"\"\"}\n",
        "        ]\n",
        "        \n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True,\n",
        "            enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
        "        )\n",
        "        \n",
        "        prompts.append(text)\n",
        "    \n",
        "    return prompts\n",
        "\n",
        "def extract_answer_from_response(response_text: str, question_data: Dict[str, Any]) -> str:\n",
        "    \"\"\"\n",
        "    Extract the answer letter (A, B, C, D, etc.) from the model response.\n",
        "    \n",
        "    Args:\n",
        "        response_text: The raw response from the model\n",
        "        question_data: The question data containing options\n",
        "    \n",
        "    Returns:\n",
        "        The answer letter (A, B, C, D, etc.) or 'X' if not found\n",
        "    \"\"\"\n",
        "    # Clean the response text\n",
        "    response_text = response_text.strip().upper()\n",
        "    \n",
        "    # Get available options from the question data\n",
        "    options = question_data.get('options', {})\n",
        "    if isinstance(options, str):\n",
        "        try:\n",
        "            options = json.loads(options)\n",
        "        except:\n",
        "            options = {}\n",
        "    \n",
        "    # Extract valid option letters (A, B, C, D, etc.)\n",
        "    valid_options = list(options.keys()) if isinstance(options, dict) else ['A', 'B', 'C', 'D', 'E']\n",
        "    valid_pattern = '|'.join(valid_options)\n",
        "    \n",
        "    # Look for patterns like \"A\", \"B\", \"Answer: A\", \"The answer is B\", etc.\n",
        "    patterns = [\n",
        "        rf'<answer>([{valid_pattern}])</answer>',  # Look for the required format first, then fallback patterns\n",
        "        rf'[aA]nswer[\\s:]*([{valid_pattern}])',\n",
        "        rf'\\b([{valid_pattern}])\\b',\n",
        "        rf'option[\\s:]*([{valid_pattern}])',\n",
        "        rf'choice[\\s:]*([{valid_pattern}])',\n",
        "    ]\n",
        "    \n",
        "    for pattern in patterns:\n",
        "        match = re.search(pattern, response_text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1).upper()\n",
        "    \n",
        "    # If no clear answer found, try to match against the actual options\n",
        "    if isinstance(options, dict):\n",
        "        for key, value in options.items():\n",
        "            if value.lower() in response_text.lower():\n",
        "                return key.upper()\n",
        "    \n",
        "    # Default to 'X' if no answer found\n",
        "    return 'X'\n",
        "\n",
        "def make_batches(questions: List[Dict[str, Any]], batch_size: int = 8) -> List[List[Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Split questions into batches for processing.\n",
        "    \n",
        "    Args:\n",
        "        questions: List of question data\n",
        "        batch_size: Number of questions per batch\n",
        "    \n",
        "    Returns:\n",
        "        List of batches, each containing a list of questions\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    for i in range(0, len(questions), batch_size):\n",
        "        batch = questions[i:i + batch_size]\n",
        "        batches.append(batch)\n",
        "    return batches\n",
        "\n",
        "def generate_completions(questions, model_name, output_filename=\"answers.jsonl\") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Generate completions for a list of questions.\"\"\"\n",
        "    batches = make_batches(questions, BATCH_SIZE)\n",
        "    print(f\"Processing {len(questions)} questions in {len(batches)} batches...\")\n",
        "    \n",
        "    results = []\n",
        "    for batch_idx, batch in tqdm(enumerate(batches), total=len(batches)):\n",
        "        print(f\"\\nProcessing batch {batch_idx + 1}/{len(batches)} ({len(batch)} questions)...\")\n",
        "        \n",
        "        try:\n",
        "            prompts = create_prompts(batch, model_name)\n",
        "            response = client.completions.create(\n",
        "                model=model_name,\n",
        "                prompt=prompts,\n",
        "                max_tokens=MAX_TOKENS_PER_QUESTION,\n",
        "                temperature=TEMPERATURE\n",
        "            )\n",
        "            \n",
        "            for i, (question_data, choice) in enumerate(zip(batch, response.choices)):\n",
        "                response_text = choice.text\n",
        "                answer_letter = extract_answer_from_response(response_text, question_data)\n",
        "                \n",
        "                result = {\n",
        "                    **question_data,\n",
        "                    'raw_response': response_text,\n",
        "                    'answer_letter': answer_letter\n",
        "                }\n",
        "                results.append(result)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_idx + 1}: {e}\")\n",
        "            for question_data in batch:\n",
        "                result = {\n",
        "                    **question_data,\n",
        "                    'raw_response': f'Error: {str(e)}',\n",
        "                    'answer_letter': 'X'\n",
        "                }\n",
        "                results.append(result)\n",
        "    \n",
        "    # Save results\n",
        "    with open(output_filename, 'w') as f:\n",
        "        for result in results:\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "    \n",
        "    print(f\"Results saved to {output_filename}\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Analyze Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sample questions using the function\n",
        "questions = load_questions(\"hackathon-train.jsonl\")\n",
        "\n",
        "# Display sample question structure\n",
        "print(f\"First question: {questions[0]['question'][:100]}...\")\n",
        "print(\"\\nSample question structure:\")\n",
        "print(json.dumps(questions[0], indent=2))\n",
        "\n",
        "# Analyze question types and option counts\n",
        "question_types = {}\n",
        "option_counts = {}\n",
        "for q in questions:\n",
        "    q_type = q.get('question_type', 'unknown')\n",
        "    question_types[q_type] = question_types.get(q_type, 0) + 1\n",
        "    \n",
        "    # Parse options to count them\n",
        "    options = q.get('options', {})\n",
        "    if isinstance(options, str):\n",
        "        try:\n",
        "            options = json.loads(options)\n",
        "        except:\n",
        "            options = {}\n",
        "    \n",
        "    if isinstance(options, dict):\n",
        "        option_count = len(options)\n",
        "        option_counts[option_count] = option_counts.get(option_count, 0) + 1\n",
        "\n",
        "print(f\"\\nQuestion types: {question_types}\")\n",
        "print(f\"Option counts: {option_counts}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Process Questions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Batch processing configuration:\")\n",
        "print(f\"  Batch size: {BATCH_SIZE}\")\n",
        "print(f\"  Max tokens per question: {MAX_TOKENS_PER_QUESTION}\")\n",
        "print(f\"  Temperature: {TEMPERATURE}\")\n",
        "print(f\"  Total questions: {len(questions)}\")\n",
        "print(f\"  Number of batches: {(len(questions) + BATCH_SIZE - 1) // BATCH_SIZE}\")\n",
        "\n",
        "# Generate completions\n",
        "results = generate_completions(questions, model_name, \"sample_answers.jsonl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Statistics (Optional)\n",
        "\n",
        "Calculate statistics about the generated answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate statistics\n",
        "total_questions = len(results)\n",
        "valid_answers = sum(1 for r in results if r['answer_letter'] in ['A', 'B', 'C', 'D', 'E'])\n",
        "correct_answers = sum(1 for r in results if r['answer_letter'] == r['answer'])\n",
        "\n",
        "print(\"\\n=== SUMMARY STATISTICS ===\")\n",
        "print(f\"Total questions processed: {total_questions}\")\n",
        "print(f\"Valid answers (A, B, C, D, E): {valid_answers} ({valid_answers/total_questions*100:.1f}%)\")\n",
        "print(f\"Correct answers: {correct_answers} ({correct_answers/total_questions*100:.1f}%)\")\n",
        "print(f\"Accuracy (of valid answers): {correct_answers/valid_answers*100:.1f}%\" if valid_answers > 0 else \"No valid answers\")\n",
        "\n",
        "# Count by question type\n",
        "question_types = {}\n",
        "for result in results:\n",
        "    q_type = result.get('question_type', 'unknown')\n",
        "    if q_type not in question_types:\n",
        "        question_types[q_type] = {'total': 0, 'correct': 0}\n",
        "    question_types[q_type]['total'] += 1\n",
        "    if result['answer_letter'] == result['answer']:\n",
        "        question_types[q_type]['correct'] += 1\n",
        "\n",
        "print(\"\\n=== BY QUESTION TYPE ===\")\n",
        "for q_type, stats in question_types.items():\n",
        "    accuracy = stats['correct'] / stats['total'] * 100 if stats['total'] > 0 else 0\n",
        "    print(f\"{q_type}: {stats['correct']}/{stats['total']} ({accuracy:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Process test questions\n",
        "test_file = \"hackathon-test.jsonl\"\n",
        "if Path(test_file).exists():\n",
        "    test_questions = load_questions(test_file)\n",
        "    test_results = generate_completions(test_questions, model_name, \"test_answers.jsonl\")\n",
        "    print(\"✅ Test questions processed!\")\n",
        "else:\n",
        "    print(f\"ℹ️  No {test_file} found in directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload Results\n",
        "\n",
        "Upload your generated answers to the leaderboard:\n",
        "\n",
        "```bash\n",
        "# Upload test answers (if generated)\n",
        "python upload_answers.py test_answers.jsonl --team-name \"test_team\" --tag \"qwen3_8b_no_tooling\"\n",
        "```\n",
        "\n",
        "## Customize Your Approach\n",
        "\n",
        "To test your own methods, simply replace the `generate_completions` function with your own logic:\n",
        "\n",
        "```python\n",
        "def my_custom_method(questions, output_filename=\"my_answers.jsonl\"):\n",
        "    # Your custom logic here\n",
        "    results = []\n",
        "    for question in questions:\n",
        "        # Process question with your method\n",
        "        answer = your_method(question)\n",
        "        results.append({\n",
        "            **question,\n",
        "            'answer_letter': answer\n",
        "        })\n",
        "    \n",
        "    # Save results\n",
        "    with open(output_filename, 'w') as f:\n",
        "        for result in results:\n",
        "            f.write(json.dumps(result) + '\\n')\n",
        "    \n",
        "    return results\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
