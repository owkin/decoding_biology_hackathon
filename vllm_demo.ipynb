{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vLLM Docker Demo\n",
    "\n",
    "This notebook demonstrates how to use the vLLM server with the OpenAI client.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. Start the vLLM Docker container:\n",
    "   ```bash\n",
    "   ./start_vllm_docker.sh\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the OpenAI Client\n",
    "\n",
    "The client will connect to the vLLM server running in Docker, which acts as drop-in replacement for OpenAI distant models callable via their API.\n",
    "\n",
    "The difference is that the vLLM server is running on your AWS instance, you can run any open weights model and it will be much faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the OpenAI client to connect to vLLM server\n",
    "# This will connect to the Docker container running on localhost:8000\n",
    "client = OpenAI(\n",
    "    api_key=\"EMPTY\",  # vLLM doesn't require authentication\n",
    "    base_url=\"http://localhost:8000/v1\",\n",
    ")\n",
    "\n",
    "# Test the connection by listing available models\n",
    "try:\n",
    "    models = list(client.models.list())\n",
    "    if models:\n",
    "        print(\"vLLM server is up and running!\")\n",
    "        print(f\"Available models: {[model.id for model in models]}\")\n",
    "    else:\n",
    "        print(\"No models available\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to vLLM server: {e}\")\n",
    "    print(\"Make sure the server is running by executing `./start_vllm_docker.sh`\")\n",
    "\n",
    "model = models[0].id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Completion Example\n",
    "\n",
    "You can use the local model is if it were served by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a single completion using the Completions API\n",
    "prompt = \"What is the role of proteins in biological systems?\"\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(\"\\nGenerating completion...\")\n",
    "\n",
    "# Generate completion\n",
    "response = client.completions.create(\n",
    "    model=model,  \n",
    "    prompt=prompt,\n",
    "    max_tokens=256,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# Extract the text from the response\n",
    "completion_text = response.choices[0].text\n",
    "\n",
    "print(f\"\\nCompletion: {completion_text}\")\n",
    "print(f\"\\nUsage: {response.usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Example\n",
    "\n",
    "Whenever possible, you should try to run multiple prompts at once. This is more and more efficient as the batch size increases, but uses more VRAM. You should be able to run hundreds or thousands of prompts at once with your setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate completions for multiple prompts\n",
    "prompts = [\n",
    "    \"Explain the process of DNA replication.\",\n",
    "    \"What are the main functions of mitochondria?\",\n",
    "    \"How do enzymes work in biological reactions?\"\n",
    "]\n",
    "\n",
    "print(\"Generating batch completions...\")\n",
    "print(f\"Processing {len(prompts)} prompts\\n\")\n",
    "\n",
    "# Generate batch completions using a loop\n",
    "response = client.completions.create(\n",
    "    model=model,\n",
    "    prompt=prompts,\n",
    "    max_tokens=200,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "# Display results\n",
    "for i, (prompt, answer) in enumerate(zip(prompts, response.choices)):\n",
    "    print(f\"\\n--- Question {i+1} ---\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Answer: {answer.text}\")\n",
    "print(f\"Tokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting with system prompt and multi-turn conversations\n",
    "\n",
    "You have the choice between using the OpenAI Chat Completions API or format the conversation using the Transformers AutoTokenizer.\n",
    "\n",
    "Using the OpenAI Chat Completions API directly with a list of messages :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a poetic biology tutor. Use analogies and paint a pretty picture.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Explain photosynthesis in simple terms.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Like a chef making a meal, a plant uses sunlight, water, and air to create food.\"},\n",
    "        {\"role\": \"user\", \"content\": \"OK, please be a little more specific. I want to learn the molecular science behind it !\"},\n",
    "]\n",
    "\n",
    "# Chat completion example\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    max_tokens=500,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Chat Response:\")\n",
    "print(f\"Assistant: {chat_response.choices[0].message.content}\")\n",
    "print(f\"\\nUsage: {chat_response.usage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to send batches of such conversations (which you should), you can use the AutoTokenizer library to format the conversation.\n",
    "\n",
    "It simply uses the correct special tokens for the model you are using and formats the conversation as one string.\n",
    "See https://huggingface.co/docs/transformers/en/chat_templating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "response = client.completions.create(\n",
    "    model=model,\n",
    "    prompt=[prompt],\n",
    "    max_tokens=200,\n",
    "    temperature=0.8\n",
    ")\n",
    "\n",
    "print(f\"Prompt: {prompt}\")\n",
    "print(f\"Answer: {answer.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions with Tool Calling\n",
    "\n",
    "If the vLLM server is started with the `enable-auto-tool-choice` option, it can generate its own tool calls when it deems appropriate.\n",
    "\n",
    "See https://docs.vllm.ai/en/stable/features/tool_calling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_temperature(location: str, unit: str = \"celsius\"):\n",
    "    \"\"\"Get current temperature at a location.\n",
    "\n",
    "    Args:\n",
    "        location: The location to get the temperature for, in the format \"City, State, Country\".\n",
    "        unit: The unit to return the temperature in. Defaults to \"celsius\". (choices: [\"celsius\", \"fahrenheit\"])\n",
    "\n",
    "    Returns:\n",
    "        the temperature, the location, and the unit in a dict\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"temperature\": 26.1,\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "    }\n",
    "\n",
    "\n",
    "tool_functions = {\"get_current_temperature\": get_current_temperature}\n",
    "\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_current_temperature\",\n",
    "        \"description\": \"Get the current temperature in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"City, State, Country, e.g., 'San Francisco, CA, USA'\"},\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "            },\n",
    "            \"required\": [\"location\", \"unit\"]\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco, CA, USA?\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "tool_call = response.choices[0].message.tool_calls[0].function\n",
    "print(f\"Function called: {tool_call.name}\")\n",
    "print(f\"Arguments: {tool_call.arguments}\")\n",
    "print(f\"Result: {tool_functions[tool_call.name](**json.loads(tool_call.arguments))}\")\n",
    "print(f\"Assistant: {response.choices[0].message.content}\")\n",
    "print(f\"\\nUsage: {response.usage}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response.choices[0].message.model_dump())\n",
    "\n",
    "def get_function_by_name(name):\n",
    "    if name == \"get_current_temperature\":\n",
    "        return get_current_temperature\n",
    "\n",
    "if tool_calls := messages[-1].get(\"tool_calls\", None):\n",
    "    for tool_call in tool_calls:\n",
    "        call_id: str = tool_call[\"id\"]\n",
    "        if fn_call := tool_call.get(\"function\"):\n",
    "            fn_name: str = fn_call[\"name\"]\n",
    "            fn_args: dict = json.loads(fn_call[\"arguments\"])\n",
    "        \n",
    "            fn_res: str = json.dumps(get_function_by_name(fn_name)(**fn_args))\n",
    "\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": fn_res,\n",
    "                \"tool_call_id\": call_id,\n",
    "            })\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=messages,\n",
    "    tools=tools,\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    max_tokens=512,\n",
    "    extra_body={\n",
    "        \"repetition_penalty\": 1.05,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **OpenAI-Compatible Interface**: Using vLLM with the standard OpenAI Python client\n",
    "2. **Client Initialization**: How to set up the OpenAI client to connect to a Docker-based vLLM server\n",
    "3. **Single Completions**: Generating responses for individual prompts using the Completions API\n",
    "4. **Batch Processing**: Efficiently processing multiple prompts at once\n",
    "5. **Chat Completions**: Using the Chat Completions API for conversational interactions\n",
    "6. **Tool Calling**: Using function calling capabilities with the Chat Completions API\n",
    "7. **Biology Applications**: Using the model for biology-related questions\n",
    "8. **Usage Statistics**: Monitoring token usage\n",
    "\n",
    "The vLLM Docker setup with OpenAI-compatible interface provides a scalable and familiar way to serve large language models for biology research and education applications. The standard OpenAI client makes it easy to integrate with existing applications and frameworks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (My Cool Project)",
   "language": "python",
   "name": "my-project-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
